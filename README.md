
# ğŸ“„ RAG Chatbot (Fully Local)

A **fully local Retrieval-Augmented Generation (RAG) chatbot** that answers questions based on custom documents using **local embeddings, FAISS vector search, and a local LLM** â€” no external APIs, no cloud dependency.

---

## ğŸš€ Features

* ğŸ“‘ PDF document ingestion
* âœ‚ï¸ Text chunking for efficient retrieval
* ğŸ”¢ **Local embeddings** using HuggingFace Sentence Transformers
* ğŸ“¦ Vector search with **FAISS**
* ğŸ¤– **Fully local LLM** using Ollama (Mistral)
* ğŸ’¬ Conversational memory
* ğŸŒ Interactive UI with Streamlit
* ğŸ”’ Works **offline**, no API keys required

---

## ğŸ§  Architecture Overview

```
PDF Documents
     â†“
Text Chunking
     â†“
Local Embeddings (Sentence-Transformers)
     â†“
FAISS Vector Store
     â†“
Retriever
     â†“
Local LLM (Ollama - Mistral)
     â†“
Streamlit Chat Interface
```

---

## ğŸ› ï¸ Tech Stack

* **Python 3.11**
* **LangChain**
* **HuggingFace Sentence Transformers**
* **FAISS**
* **Ollama (Mistral LLM)**
* **Streamlit**

---

## ğŸ“ Project Structure

```
rag-chatbot/
â”‚â”€â”€ app.py              # Streamlit chatbot UI
â”‚â”€â”€ ingest.py           # Document ingestion & vector creation
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore
â”‚â”€â”€ data/
â”‚   â””â”€â”€ sample.pdf      # Input document
â”‚â”€â”€ vectorstore/        # FAISS index (generated)
â”‚â”€â”€ venv/
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Add Your Document

Place your PDF inside the `data/` folder and rename it to:

```
sample.pdf
```

---

### 5ï¸âƒ£ Create Vector Store (Local Embeddings)

```bash
python ingest.py
```

You should see:

```
âœ… Vector store created successfully!
```

---

### 6ï¸âƒ£ Install Ollama (Local LLM)

Download and install Ollama:
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

Pull the Mistral model:

```bash
ollama pull mistral
```

---

### 7ï¸âƒ£ Run the Chatbot

```bash
streamlit run app.py
```

Open the browser and start asking questions about your document.

---

## ğŸ’¡ Example Questions

* â€œWhat is this document about?â€
* â€œSummarize the key pointsâ€
* â€œExplain this concept in simple termsâ€
* â€œWhat are the conclusions?â€

---

## ğŸ”® Future Enhancements

* Source citation display
* Multi-PDF support
* Chat history UI
* Dockerization
* FastAPI backend

---

