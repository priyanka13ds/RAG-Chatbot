# ğŸ“„ Retrieval-Augmented Generation (RAG) Chatbot

A production-style Retrieval-Augmented Generation (RAG) chatbot for document-grounded question answering.  
This project demonstrates an end-to-end GenAI pipeline using local embeddings, vector search, and a locally hosted large language model to enable efficient, privacy-preserving inference on custom documents.

---

## âš¡ Quick Start (TL;DR)

```bash
git clone https://github.com/priyanka13ds/RAG-Chatbot.git
cd RAG-Chatbot
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
python ingest.py
ollama pull mistral
streamlit run app.py
```

## Key Features

* PDF-based knowledge ingestion
* Semantic text chunking for improved retrieval
* Local embeddings using HuggingFace Sentence Transformers
* FAISS-based vector similarity search
* Locally hosted LLM via Ollama (Mistral)
* Multi-turn conversational memory
* Interactive Streamlit-based UI
* Fully offline and privacy-preserving inference

---

## ğŸ§  Architecture Overview

```
User Document (PDF)
        â†“
Text Chunking
        â†“
Sentence Embeddings (Local)
        â†“
FAISS Vector Store
        â†“
Semantic Retriever
        â†“
Local LLM (Ollama â€“ Mistral)
        â†“
Streamlit Chat Interface
```

---

## ğŸ› ï¸ Technology Stack

- **Language:** Python 3.11  
- **Orchestration:** LangChain  
- **Embeddings:** HuggingFace Sentence Transformers  
- **Vector Store:** FAISS  
- **LLM Runtime:** Ollama (Mistral)  
- **Frontend:** Streamlit  

---

## ğŸ“ Project Structure

```
rag-chatbot/
â”‚â”€â”€ app.py              # Streamlit chatbot UI
â”‚â”€â”€ ingest.py           # Document ingestion & vector creation
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore
â”‚â”€â”€ data/
â”‚   â””â”€â”€ sample.pdf      # Input document
â”‚â”€â”€ vectorstore/        # FAISS index (generated)
â”‚â”€â”€ venv/
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
```

---

### 2ï¸âƒ£ Create and Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Add Input Document

Place a PDF file inside the data/ directory and rename it to:

```
sample.pdf
```

---

### 5ï¸âƒ£ Build the Vector Store

Generate embeddings and create the FAISS index:

```bash
python ingest.py
```

Expected output:

```
âœ… Vector store created successfully!
```

---

### 6ï¸âƒ£ Install and Configure Local LLM (Ollama)

Download and install Ollama:
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

Pull the Mistral model:

```bash
ollama pull mistral
```

---

### 7ï¸âƒ£ Run the Chatbot

```bash
streamlit run app.py
```

The application will open in your browser, allowing you to interactively query your document.

---

## ğŸ’¡ Sample Queries
* What is the main topic of this document?
* Summarize the key points.
* Explain this concept in simple terms.
* What conclusions are presented?

---

## ğŸ¯ Why This Project Matters

This project demonstrates hands-on experience with:

- Retrieval-Augmented Generation (RAG) system design
- Vector similarity search for semantic information retrieval
- Integration of locally hosted large language models for cost-efficient inference
- End-to-end GenAI application development, from data ingestion to user-facing interface

It reflects real-world engineering considerations such as data privacy, reproducibility, and modular system design.

---

## ğŸ”® Future Enhancements

- Source citation and document traceability  
- Multi-document ingestion and indexing  
- Enhanced chat history visualization  
- Dockerized deployment  
- FastAPI backend for scalable serving  

---

